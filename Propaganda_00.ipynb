{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import All Packages\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    import random\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import string\n",
    "    from string import digits\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import classification_report\n",
    "    import transformers\n",
    "    from transformers import AutoModel, BertTokenizerFast\n",
    "    from ipywidgets import IntProgress\n",
    "    from tqdm import tqdm\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import BERT Model, BERT Tokenizer and Torch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# import Torch\n",
    "device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predefined CLass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Propaganda:\n",
    "    NEGATIVE = 0\n",
    "    POSITIVE = 1\n",
    "\n",
    "class Review:\n",
    "    def __init__(self,sentence,SUBJprop):\n",
    "        self.sentence = sentence\n",
    "        self.SUBJprop = SUBJprop\n",
    "        self.propaganda = SUBJprop\n",
    "\n",
    "\n",
    "class ReviewContainer:\n",
    "    def __init__(self,reviews):\n",
    "        self.reviews = reviews\n",
    "\n",
    "    def get_sentence(self):\n",
    "        return [x.sentence for x  in self.reviews]\n",
    "\n",
    "    def get_propaganda(self):\n",
    "        return [int(x.propaganda) for x in self.reviews]\n",
    "\n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), self.reviews))\n",
    "        positive = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), self.reviews))\n",
    "        negative_shrunk = negative[:len(positive)]\n",
    "        self.reviews = positive + negative_shrunk\n",
    "        random.shuffle(self.reviews)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train dataset into train, validation and test sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# step 2.1: Load Data\n",
    "reviews = []\n",
    "data  = pd.read_excel('Data/finalDataset.xlsx', engine='openpyxl')\n",
    "df = pd.DataFrame(data.astype(str) , columns = ['Sentence','SUBJprop'])\n",
    "# iterate elements of attribute \"Sentence\" and \"SUBJprop\" and push to the array \"reviews\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    prop = row['SUBJprop']\n",
    "    reviews.append(Review(sentence,prop))\n",
    "\n",
    "print(\"Total Rows:\")\n",
    "print(len(reviews))\n",
    "print(\"Total Positive:\")\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))))\n",
    "print(\"Total Negative:\")\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "neg_prop = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))\n",
    "pos_prop = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))\n",
    "########################################################################################\n",
    "#split trainig and DevTest dataset\n",
    "neg_train, neg_devtest  = train_test_split(neg_prop , train_size=0.7, shuffle= False )\n",
    "pos_train, pos_devtest = train_test_split(pos_prop , train_size=0.7, shuffle= False )\n",
    "########################################################################################\n",
    "#prepare training dataset\n",
    "train = neg_train + pos_train\n",
    "#random.shuffle(train)\n",
    "########################################################################################\n",
    "#prepare development and test dataset\n",
    "neg_dev, neg_test = train_test_split(neg_devtest , train_size=0.5, shuffle= False )\n",
    "pos_dev, pos_test = train_test_split(pos_devtest , train_size=0.5, shuffle= False )\n",
    "\n",
    "dev = neg_dev + pos_dev\n",
    "#random.shuffle(dev)\n",
    "\n",
    "test = neg_test + pos_test\n",
    "#random.shuffle(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# step 3: Seperate the attribute, originally our array has text and score. we want them to be a seperate array\n",
    "train_container = ReviewContainer(train)\n",
    "train_container.evenly_distribute()\n",
    "\n",
    "train_text = train_container.get_sentence()   \n",
    "train_labels = train_container.get_propaganda() \n",
    "\n",
    "dev_text = [x.sentence for x in dev]\n",
    "dev_labels = [int(x.propaganda) for x in dev]\n",
    "\n",
    "test_text = [x.sentence for x in test]\n",
    "test_labels = [int(x.propaganda) for x in test]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Total Train Records:')\n",
    "print(len(train_text))\n",
    "print('Negative Train Records:')\n",
    "print(train_labels.count(0))\n",
    "print('Positive Train Records:')\n",
    "print(train_labels.count(1))\n",
    "print('\\n')\n",
    "print('Total Dev Records:')\n",
    "print(len(dev_text))\n",
    "print('Negative Dev Records:')\n",
    "print(dev_labels.count(0))\n",
    "print('Positive Dev Records:')\n",
    "print(dev_labels.count(1))\n",
    "print('\\n')\n",
    "print('Total Test Records:')\n",
    "print(len(test_text))\n",
    "print('Negative Test Records:')\n",
    "print(test_labels.count(0))\n",
    "print('Positive Test Records:')\n",
    "print(test_labels.count(1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save splited Data to seprate excel files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create Excel file of train dataset\n",
    "train_df = pd.DataFrame(train_text, columns=[\"Sentence\"])\n",
    "train_df['SUBJprop'] = train_labels\n",
    "train_df.to_excel(\"./Data/trainDataset.xlsx\")\n",
    "# Create Excel file of dev dataset\n",
    "dev_df = pd.DataFrame(dev_text, columns=[\"Sentence\"])\n",
    "dev_df['SUBJprop'] = dev_labels\n",
    "dev_df['Tanbih'] = \"\"\n",
    "dev_df.to_excel(\"./Data/devDataset.xlsx\")\n",
    "# Create Excel file of test dataset\n",
    "test_df = pd.DataFrame(test_text, columns=[\"Sentence\"])\n",
    "test_df['SUBJprop'] = test_labels\n",
    "test_df.to_excel(\"./Data/testDataset.xlsx\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization and Filtering Punctuation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizerNLTK = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "train_text_tokenized = []\n",
    "dev_text_tokenized = []\n",
    "test_text_tokenized = []\n",
    "\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "    train_text_tokenized.append(tokenizerNLTK.tokenize(train_text[i])) \n",
    "\n",
    "for i in range(len(dev_text)):\n",
    "    dev_text_tokenized.append(tokenizerNLTK.tokenize(dev_text[i])) \n",
    "\n",
    "for i in range(len(test_text)):\n",
    "    test_text_tokenized.append(tokenizerNLTK.tokenize(test_text[i]))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Before Sentence Tokenization:\")\n",
    "print(train_text[0:3])\n",
    "print(\"\\nAfter Sentence Tokenization:\")\n",
    "print(train_text_tokenized[0:3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Repetition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train_text2=[]\n",
    "# SentimentWords\n",
    "\n",
    "# def checkIfDuplicates(listOfElems):\n",
    "#     if len(listOfElems) == len(set(listOfElems)):\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True\n",
    "\n",
    "# for sentence in train_text1:\n",
    "#     temp=[]\n",
    "#     if checkIfDuplicates(sentence):\n",
    "#         print(sentence)\n",
    "\n",
    "\n",
    "\n",
    "    # for word in sentence:\n",
    "    #     temp.append(word)\n",
    "\n",
    "    # train_text2.append(temp)\n",
    "\n",
    "# print(train_text2[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Dictionaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# step 2.2: Load Sentimental Data\n",
    "SentimentWords= []\n",
    "SentimentValue= []\n",
    "\n",
    "\n",
    "Sentimentdata  = pd.read_excel('Data/SentimentalWords.xlsx', engine='openpyxl')\n",
    "df = pd.DataFrame(Sentimentdata.astype(str) , columns = ['word','value'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    word = row['word']\n",
    "    value = row['value']\n",
    "    SentimentWords.append(word)\n",
    "    SentimentValue.append(value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Removing stop words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "\n",
    "# Create list of word tokens after removing stopwords in train_text\n",
    "train_text1=[]\n",
    "for sentence in train_text_tokenized:\n",
    "    temp=[]\n",
    "    for word in sentence:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            temp.append(word) \n",
    "    train_text1.append(temp)\n",
    "\n",
    "# print(train_text1[:100])\n",
    "# print(\"*****************************************************\")\n",
    "\n",
    "# Create list of word tokens after removing stopwords in dev_text\n",
    "dev_text1 =[]\n",
    "\n",
    "for sentence in dev_text_tokenized:\n",
    "    temp=[]\n",
    "    for word in sentence:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            temp.append(word) \n",
    "    dev_text1.append(temp)\n",
    "    \n",
    "# print(dev_text1[:100])\n",
    "# print(\"*****************************************************\")\n",
    "\n",
    "# # Create list of word tokens after removing stopwords in test_text\n",
    "test_text1 =[]\n",
    "\n",
    "for sentence in test_text_tokenized:\n",
    "    temp=[]\n",
    "    for word in sentence:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            temp.append(word)\n",
    "    test_text1.append(temp)\n",
    "# print(test_text1[:100])\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_labels[8])\n",
    "print('Before Removing Stopwords:')    \n",
    "print(train_text_tokenized[8])\n",
    "print('\\nAfter Removing Stopwords:') \n",
    "print(train_text1[8])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Duplicate Sentimenal words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_text2=[]\n",
    "\n",
    "for sentence in train_text1:\n",
    "    temp=[]\n",
    "    for word in sentence:\n",
    "        temp.append(word)\n",
    "        for SentimentWord in SentimentWords:\n",
    "            if word.lower()==SentimentWord or word==SentimentWord:\n",
    "                temp.append(word)\n",
    "                temp.append(word)\n",
    "    train_text2.append(temp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(\"Propoganda:\")\n",
    "# print(train_labels[:64])\n",
    "print('Before Duplication:')\n",
    "print(train_text1[64])\n",
    "print('\\After Duplication:')\n",
    "print(train_text2[64])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Detokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_text_detokenized=[]\n",
    "for i in range(len(train_text2)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(train_text2[i])):\n",
    "        temp=temp + \" \" + train_text2[i][j]        \n",
    "    train_text_detokenized.append(temp)\n",
    "# print(train_text_detokenized[:100])\n",
    "\n",
    "dev_text_detokenized=[]\n",
    "for i in range(len(dev_text1)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(dev_text1[i])):\n",
    "        temp=temp + \" \" + dev_text1[i][j]        \n",
    "    dev_text_detokenized.append(temp)\n",
    "# print(dev_text_detokenized[:100])\n",
    "\n",
    "test_text_detokenized=[]\n",
    "for i in range(len(test_text1)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(test_text1[i])):\n",
    "        temp=temp + \" \" + test_text1[i][j]        \n",
    "    test_text_detokenized.append(temp)\n",
    "# print(test_text_detokenized[:100])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_text_detokenized[60:64])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_seq_len = 119\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text_detokenized,\n",
    "    # train_text2,\n",
    "    # is_split_into_words=True,\n",
    "    # max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    # padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    dev_text_detokenized,\n",
    "    # dev_text1,\n",
    "    # is_split_into_words=True,\n",
    "    # max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    # padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text_detokenized,\n",
    "    # test_text1,\n",
    "    # is_split_into_words=True,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    # padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert Integer Sequences to Tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels)\n",
    "\n",
    "\n",
    "\n",
    "# for validation set\n",
    "dev_seq = torch.tensor(tokens_val['input_ids'])\n",
    "dev_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "dev_y = torch.tensor(dev_labels)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_seq[1])\n",
    "print(train_mask[1])\n",
    "print(train_y[1])\n",
    "print(train_labels[1])\n",
    "# print(test_seq)\n",
    "# print(test_mask)\n",
    "# print(test_y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='rbf',C=1, probability=True, gamma=0.00000001)\n",
    "clf_svm.fit(train_seq, train_y)\n",
    "\n",
    "# clf_svm.predict(train_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tune using Training Data for SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "train_seq, test_seq, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(train_seq, train_y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = test_y, clf.predict(test_seq)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_svm_predict = clf_svm.predict(test_seq)\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[2000:2020])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_svm_predict[2000:2020])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[:20])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_svm_predict[:20])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier(random_state=0)\n",
    "clf_dec.fit(train_seq , train_y)\n",
    "\n",
    "# clf_dec.predict(test_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_dec_predict = clf_dec.predict(test_seq)\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[2000:2050])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_dec_predict[2000:2050])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[:50])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_dec_predict[:50])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB(var_smoothing=0.2848035868435802)\n",
    "clf_gnb.fit(train_seq , train_y)\n",
    "\n",
    "# clf_gnb.predict(test_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tune using Training Data for Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)\n",
    "nbModel_grid.fit(train_seq, train_y)\n",
    "print(nbModel_grid.best_estimator_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_gnb_predict = clf_gnb.predict(test_seq)\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[2000:2050])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_gnb_predict[2000:2050])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[:10])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_gnb_predict[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_seq, train_y)\n",
    "\n",
    "# clf_log.predict(test_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_log_predict = clf_log.predict(test_seq)\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[2000:2010])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_log_predict[2000:2010])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[:10])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_log_predict[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## F1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(f1_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Decision Tree\n",
    "print(f1_score(test_y,clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Naive Bayes\n",
    "print(f1_score(test_y,clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Logistic Regression\n",
    "print(f1_score(test_y,clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For Support Vector Machine\n",
    "print(clf_svm.score(test_seq,test_y))\n",
    "# For Decision Tree\n",
    "print(clf_dec.score(test_seq,test_y))\n",
    "# For Decision Naive Bayes\n",
    "print(clf_gnb.score(test_seq,test_y))\n",
    "# For Logistic Regression\n",
    "print(clf_log.score(test_seq,test_y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Precision"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(precision_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(precision_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(precision_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(precision_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recall"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(recall_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(recall_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(recall_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(recall_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VIP : Predict_Proba using Threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Evaluation of Above Threshold SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AboveThresholdsvm = [] \n",
    "AboveThresholdsvmbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdsvm.append(test_text[i])\n",
    "    AboveThresholdsvmbrop.append(test_labels[i])\n",
    "\n",
    "print(\"Number of records above threshold SVM:\")\n",
    "print(len(AboveThresholdsvm))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Evaluation of Above Threshold Desicion Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AboveThresholdDT = [] \n",
    "AboveThresholdDTbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdDT.append(test_text[i])\n",
    "    AboveThresholdDTbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "print(\"Number of records above threshold Desicion Tree:\")\n",
    "print(len(AboveThresholdDT))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Evaluation of Above Threshold Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AboveThresholdGNB = [] \n",
    "AboveThresholdGNBbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdGNB.append(test_text[i])\n",
    "    AboveThresholdGNBbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "print(\"Number of records above threshold Logistic Regression:\")\n",
    "print(len(AboveThresholdGNB))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Evaluation of Above Threshold Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AboveThresholdLR = [] \n",
    "AboveThresholdLRbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdLR.append(test_text[i])\n",
    "    AboveThresholdLRbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "print(\"Number of records above threshold Logistic Regression:\")\n",
    "print(len(AboveThresholdLR))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(clf_log.predict_proba(test_seq[2002].reshape(1, -1))[0][1])\n",
    "print(clf_log.predict(test_seq[2002].reshape(1, -1)))\n",
    "print(test_labels[2002])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize Output above threshod"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Bert\n",
    "tokens_testsvm = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdsvm,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqsvm = torch.tensor(tokens_testsvm['input_ids'])\n",
    "test_masksvm = torch.tensor(tokens_testsvm['attention_mask'])\n",
    "test_ysvm = torch.tensor(AboveThresholdsvmbrop)\n",
    "\n",
    "tokens_testDT = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdDT,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqDT = torch.tensor(tokens_testDT['input_ids'])\n",
    "test_maskDT = torch.tensor(tokens_testDT['attention_mask'])\n",
    "test_yDT = torch.tensor(AboveThresholdDTbrop)\n",
    "\n",
    "tokens_testGNB = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdGNB,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqGNB = torch.tensor(tokens_testGNB['input_ids'])\n",
    "test_maskGNB = torch.tensor(tokens_testGNB['attention_mask'])\n",
    "test_yGNB = torch.tensor(AboveThresholdGNBbrop)\n",
    "\n",
    "tokens_testLR = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdLR,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqLR = torch.tensor(tokens_testLR['input_ids'])\n",
    "test_maskLR = torch.tensor(tokens_testLR['attention_mask'])\n",
    "test_yLR = torch.tensor(AboveThresholdLRbrop)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation Above Threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#F1\n",
    "print(\"F1 SVM more than 60% threshold\")\n",
    "print(f1_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 DT more than 60% threshold\")\n",
    "print(f1_score(test_yDT, clf_log.predict(test_seqDT),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 GNB more than 60% threshold\")\n",
    "print(f1_score(test_yGNB, clf_log.predict(test_seqGNB),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 LR more than 60% threshold\")\n",
    "print(f1_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Accuracy\n",
    "# For Support Vector Machine\n",
    "print(\"Accuracy SVM more than 60% threshold\")\n",
    "print(clf_svm.score(test_seqsvm,test_ysvm))\n",
    "\n",
    "print(\"Accuracy DT more than 60% threshold\")\n",
    "print(clf_dec.score(test_seqDT,test_yDT))\n",
    "\n",
    "print(\"Accuracy GNB more than 60% threshold\")\n",
    "print(clf_gnb.score(test_seqGNB,test_yGNB))\n",
    "\n",
    "print(\"Accuracy LR more than 60% threshold\")\n",
    "print(clf_log.score(test_seqLR,test_yLR))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "# For Support Vector Machine\n",
    "print(\"Precision SVM more than 60% threshold\")\n",
    "print(precision_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision DT more than 60% threshold\")\n",
    "print(precision_score(test_yDT, clf_log.predict(test_seqDT),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision GNB more than 60% threshold\")\n",
    "print(precision_score(test_yGNB, clf_log.predict(test_seqGNB),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision LR more than 60% threshold\")\n",
    "print(precision_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Recall SVM more than 60% threshold\")\n",
    "print(recall_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Recall DT more than 60% threshold\")\n",
    "print(recall_score(test_yDT, clf_log.predict(test_seqDT),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Recall GNB more than 60% threshold\")\n",
    "print(recall_score(test_yGNB, clf_log.predict(test_seqGNB),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Recall LR more than 60% threshold\")\n",
    "print(recall_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Output=[]\n",
    "for i in range(len(test_seq)):\n",
    "  Vote=0\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "    Vote=Vote+1\n",
    "  elif clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "    Vote=Vote+1\n",
    "  elif clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "    Vote=Vote+1\n",
    "  elif clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "    Vote=Vote+1\n",
    "  if (Vote>=1):\n",
    "    Output.append(\"Propaganda\")\n",
    "  else:\n",
    "    Output.append(\"nonPropaganda\")\n",
    "\n",
    "# for i in range(len(test_seq)):\n",
    "#   Vote=0\n",
    "#   if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "#     Vote=Vote+1\n",
    "#   if (Vote==1):\n",
    "#     Output.append(\"Propaganda\")\n",
    "#   else:\n",
    "#     Output.append(\"nonPropaganda\")\n",
    "\n",
    "# print(Output.count(Propaganda))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(2000,2100):\n",
    "    print(test[i].SUBJprop, end= '')\n",
    "\n",
    "print('\\n')\n",
    "for i in range(2000,2100):\n",
    "    x = clf_svm.predict(test_seq[i].reshape(1, -1))\n",
    "    print(x, end='')\n",
    "\n",
    "# print(len(test))\n",
    "# print(len(test_seq))\n",
    "\n",
    "# x = clf_svm.predict(test_seq[2].reshape(1, -1))\n",
    "# print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CountPropaganda=0\n",
    "for i in range(len(Output)):\n",
    "    if (Output[i]==\"nonPropaganda\"):\n",
    "        CountPropaganda=CountPropaganda+1\n",
    "\n",
    "print(\"number of propandas in output:\")\n",
    "print(CountPropaganda)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}