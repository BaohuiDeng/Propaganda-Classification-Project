{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Transformers Library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: Predefined CLass\n",
    "import random\n",
    "\n",
    "class Propaganda:\n",
    "    NEGATIVE = 0\n",
    "    POSITIVE = 1\n",
    "\n",
    "class Review:\n",
    "    def __init__(self,sentence,SUBJprop):\n",
    "        self.sentence = sentence\n",
    "        self.SUBJprop = SUBJprop\n",
    "        self.propaganda = self.get_propaganda()        \n",
    "\n",
    "    def get_propaganda(self):\n",
    "        if int(self.SUBJprop) <=3 :\n",
    "            return Propaganda.NEGATIVE\n",
    "        else:\n",
    "            return Propaganda.POSITIVE\n",
    "\n",
    "\n",
    "class ReviewContainer:\n",
    "    def __init__(self,reviews):\n",
    "        self.reviews = reviews\n",
    "\n",
    "    def get_sentence(self):\n",
    "        return [x.sentence for x  in self.reviews]\n",
    "\n",
    "    def get_propaganda(self):\n",
    "        return [x.propaganda for x in self.reviews]\n",
    "\n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.propaganda == Propaganda.NEGATIVE, self.reviews))\n",
    "        positive = list(filter(lambda x: x.propaganda == Propaganda.POSITIVE, self.reviews))\n",
    "        negative_shrunk = negative[:len(positive)]\n",
    "        self.reviews = positive + negative_shrunk\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Rows:\n16774\nTotal Positive:\n3780\nTotal Negative:\n12994\n"
     ]
    }
   ],
   "source": [
    "# step 2: Load Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "\n",
    "\n",
    "reviews = []\n",
    "data  = pd.read_excel('Data/finalDataset.xlsx', engine='openpyxl')\n",
    "df = pd.DataFrame(data.astype(str) , columns = ['Sentence','SUBJprop'])\n",
    "# iterate elements of attribute \"Sentence\" and \"SUBJprop\" and push to the array \"reviews\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    prop = row['SUBJprop']\n",
    "    reviews.append(Review(sentence,prop))\n",
    "\n",
    "\n",
    "print(\"Total Rows:\")\n",
    "print(len(reviews))\n",
    "print(\"Total Positive:\")\n",
    "print(len(list(filter(lambda x: x.propaganda == Propaganda.POSITIVE, reviews))))\n",
    "print(\"Total Negative:\")\n",
    "print(len(list(filter(lambda x: x.propaganda == Propaganda.NEGATIVE, reviews))))\n",
    "\n",
    "# print(reviews[0].getSUBJprop)\n",
    "# sentenceArray = [ x.sentence for x  in reviews] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "miley and liam fighting false rumors swirl that theyre in a feud over a supposed prenup\nPietro Card.\nNothing is said of increased intensity of devotion to the charism, study of the foundress or returning to sources, still less of strengthening the Carmelites’ traditional independent self-governance or reconsidering the wisdom of “the path taken”.\n2516\n"
     ]
    }
   ],
   "source": [
    "# step 2: Prep Data (split into train and test set)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "neg_prop = list(filter(lambda x: x.propaganda == Propaganda.NEGATIVE, reviews))\n",
    "pos_prop = list(filter(lambda x: x.propaganda == Propaganda.POSITIVE, reviews))\n",
    "\n",
    "########################################################################################\n",
    "#split trainig and DevTest dataset\n",
    "neg_train, neg_devtest  = train_test_split(neg_prop , train_size=0.7 ,shuffle=False)\n",
    "pos_train, pos_devtest = train_test_split(pos_prop , train_size=0.7,shuffle = False)\n",
    "########################################################################################\n",
    "#prepare training dataset\n",
    "train = neg_train + pos_train\n",
    "#random.shuffle(train)\n",
    "print(train[0].sentence)\n",
    "########################################################################################\n",
    "#prepare development and test dataset\n",
    "neg_dev, neg_test = train_test_split(neg_devtest , train_size=0.5,shuffle = False )\n",
    "pos_dev, pos_test = train_test_split(pos_devtest , train_size=0.5,shuffle = False)\n",
    "\n",
    "dev = neg_dev + pos_dev\n",
    "#random.shuffle(dev)\n",
    "print(dev[0].sentence)\n",
    "test = neg_test + pos_test\n",
    "#random.shuffle(test)\n",
    "print(test[0].sentence)\n",
    "print(len(dev))\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: Seperate the attribute, originally our array has text and score. we want them to be a seperate array\n",
    "train_container = ReviewContainer(train)\n",
    "train_container.evenly_distribute()\n",
    "\n",
    "train_text = train_container.get_sentence()   \n",
    "train_labels = train_container.get_propaganda() \n",
    "\n",
    "dev_text = [x.sentence for x in dev]\n",
    "dev_labels = [x.propaganda for x in dev]\n",
    "\n",
    "test_text = [x.sentence for x in test]\n",
    "test_labels = [x.propaganda for x in test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT Model and BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tanna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    dev_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Integer Sequences to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[  101,  2498,  2003,  ...,  2179,  8303,   102],\n        [  101,  5262,  1010,  ..., 12645, 10462,   102],\n        [  101,  1996,  3189,  ...,  2871,  2086,   102],\n        ...,\n        [  101,  1996,  2155,  ...,  2214,  2775,   102],\n        [  101,  2019,  4358,  ...,  1010,  2429,   102],\n        [  101,  1996,  7865,  ..., 11595,  1010,   102]])\ntensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]])\ntensor([0, 0, 0,  ..., 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels)\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels)\n",
    "\n",
    "print(test_seq)\n",
    "print(test_mask)\n",
    "print(test_y)"
   ]
  },
  {
   "source": [
    "# Classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='rbf',C=1, probability=True)\n",
    "clf_svm.fit(train_seq, train_y)\n",
    "\n",
    "clf_svm.predict(test_seq)"
   ]
  },
  {
   "source": [
    "## Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_seq , train_y)\n",
    "\n",
    "clf_dec.predict(test_seq)"
   ]
  },
  {
   "source": [
    "## Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(train_seq , train_y)\n",
    "\n",
    "clf_gnb.predict(test_seq)"
   ]
  },
  {
   "source": [
    "## Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_seq, train_y)\n",
    "\n",
    "clf_log.predict(test_seq)"
   ]
  },
  {
   "source": [
    "# Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# F1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.33864916 0.51412819]\n[0.3557371  0.25886373]\n[0.34716644 0.47619048]\n[0.33349034 0.51391274]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(f1_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Decision Tree\n",
    "print(f1_score(test_y,clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Naive Bayes\n",
    "print(f1_score(test_y,clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Logistic Regression\n",
    "print(f1_score(test_y,clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ]
  },
  {
   "source": [
    "## Mean Accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4398092967818832\n0.31068732618196265\n0.4187524831148192\n0.4378228049264998\n"
     ]
    }
   ],
   "source": [
    "# For Support Vector Machine\n",
    "print(clf_svm.score(test_seq,test_y))\n",
    "# For Decision Tree\n",
    "print(clf_dec.score(test_seq,test_y))\n",
    "# For Decision Naive Bayes\n",
    "print(clf_gnb.score(test_seq,test_y))\n",
    "# For Logistic Regression\n",
    "print(clf_log.score(test_seq,test_y))"
   ]
  },
  {
   "source": [
    "## Precision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.23067093 0.78361345]\n[0.22530574 0.77493606]\n[0.23237754 0.78884935]\n[0.22750643 0.77835588]\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(precision_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(precision_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(precision_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(precision_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "source": [
    "## Recall"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.6366843 0.3825641]\n[0.84479718 0.15538462]\n[0.68606702 0.34102564]\n[0.62433862 0.38358974]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(recall_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(recall_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(recall_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(recall_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "source": [
    "# VIP : Predict_Proba using Threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "238\n"
     ]
    }
   ],
   "source": [
    "AboveThresholdsvm = [] \n",
    "AboveThresholdsvmbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdsvm.append(test_text[i])\n",
    "    AboveThresholdsvmbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "print(len(AboveThresholdsvm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "238\n"
     ]
    }
   ],
   "source": [
    "AboveThresholdLR = [] \n",
    "AboveThresholdLRbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdLR.append(test_text[i])\n",
    "    AboveThresholdLRbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "print(len(AboveThresholdsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert\n",
    "tokens_testsvm = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdsvm,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqsvm = torch.tensor(tokens_testsvm['input_ids'])\n",
    "test_masksvm = torch.tensor(tokens_testsvm['attention_mask'])\n",
    "test_ysvm = torch.tensor(AboveThresholdsvmbrop)\n",
    "\n",
    "tokens_testLR = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdLR,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "\n",
    "test_seqLR = torch.tensor(tokens_testLR['input_ids'])\n",
    "test_maskLR = torch.tensor(tokens_testLR['attention_mask'])\n",
    "test_yLR = torch.tensor(AboveThresholdLRbrop)"
   ]
  },
  {
   "source": [
    "## Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 SVM more than 60% threshold\n[0.23880597 0.70175439]\nF1 LR more than 60% threshold\n[0.25       0.80239521]\n"
     ]
    }
   ],
   "source": [
    "#F1\n",
    "print(\"F1 SVM more than 60% threshold\")\n",
    "print(f1_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 LR more than 60% threshold\")\n",
    "print(f1_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy SVM more than 60% threshold\n0.5714285714285714\nAccuracy LR more than 60% threshold\n0.6872037914691943\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "# For Support Vector Machine\n",
    "print(\"Accuracy SVM more than 60% threshold\")\n",
    "print(clf_svm.score(test_seqsvm,test_ysvm))\n",
    "print(\"Accuracy LR more than 60% threshold\")\n",
    "print(clf_log.score(test_seqLR,test_yLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision SVM more than 60% threshold\n[0.18181818 0.8       ]\nPrecision LR more than 60% threshold\n[0.24444444 0.80722892]\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "# For Support Vector Machine\n",
    "print(\"Precision SVM more than 60% threshold\")\n",
    "print(precision_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "print(\"Precision LR more than 60% threshold\")\n",
    "print(precision_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision SVM more than 60% threshold\n[0.34782609 0.625     ]\nPrecision LR more than 60% threshold\n[0.25581395 0.79761905]\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Precision SVM more than 60% threshold\")\n",
    "print(recall_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision LR more than 60% threshold\")\n",
    "print(recall_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "030fe2e537001a0f06a091970833f1c1ec5f59e52d4488afbdc2cc8dde6768f5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}