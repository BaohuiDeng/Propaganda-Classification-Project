{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Packages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import random\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import string\n",
    "    from string import digits\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import classification_report\n",
    "    import transformers\n",
    "    from transformers import AutoModel, BertTokenizerFast\n",
    "    from ipywidgets import IntProgress\n",
    "    from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT Model, BERT Tokenizer and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# import Torch\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "source": [
    "# Predefined CLass"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Propaganda:\n",
    "    NEGATIVE = 0\n",
    "    POSITIVE = 1\n",
    "class Review:\n",
    "    def __init__(self,sentence,SUBJprop):\n",
    "        self.sentence = sentence\n",
    "        self.SUBJprop = SUBJprop\n",
    "        self.propaganda = SUBJprop\n",
    "\n",
    "\n",
    "class ReviewContainer:\n",
    "    def __init__(self,reviews):\n",
    "        self.reviews = reviews\n",
    "\n",
    "    def get_sentence(self):\n",
    "        return [x.sentence for x  in self.reviews]\n",
    "\n",
    "    def get_propaganda(self):\n",
    "        return [int(x.propaganda) for x in self.reviews]\n",
    "\n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), self.reviews))\n",
    "        positive = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), self.reviews))\n",
    "        negative_shrunk = negative[:len(positive)]\n",
    "        self.reviews = positive + negative_shrunk\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Rows:\n14058\nTotal Positive:\n3904\nTotal Negative:\n10154\n"
     ]
    }
   ],
   "source": [
    "# step 2: Load Data\n",
    "reviews = []\n",
    "data  = pd.read_excel('Data/finalDataset.xlsx', engine='openpyxl')\n",
    "df = pd.DataFrame(data.astype(str) , columns = ['Sentence','SUBJprop'])\n",
    "# iterate elements of attribute \"Sentence\" and \"SUBJprop\" and push to the array \"reviews\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    prop = row['SUBJprop']\n",
    "    reviews.append(Review(sentence,prop))\n",
    "\n",
    "\n",
    "print(\"Total Rows:\")\n",
    "print(len(reviews))\n",
    "print(\"Total Positive:\")\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))))\n",
    "print(\"Total Negative:\")\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_prop = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))\n",
    "pos_prop = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))\n",
    "########################################################################################\n",
    "#split trainig and DevTest dataset\n",
    "neg_train, neg_devtest  = train_test_split(neg_prop , train_size=0.7, shuffle= False )\n",
    "pos_train, pos_devtest = train_test_split(pos_prop , train_size=0.7, shuffle= False )\n",
    "########################################################################################\n",
    "#prepare training dataset\n",
    "train = neg_train + pos_train\n",
    "#random.shuffle(train)\n",
    "########################################################################################\n",
    "#prepare development and test dataset\n",
    "neg_dev, neg_test = train_test_split(neg_devtest , train_size=0.5, shuffle= False )\n",
    "pos_dev, pos_test = train_test_split(pos_devtest , train_size=0.5, shuffle= False )\n",
    "\n",
    "dev = neg_dev + pos_dev\n",
    "#random.shuffle(dev)\n",
    "\n",
    "test = neg_test + pos_test\n",
    "#random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: Seperate the attribute, originally our array has text and score. we want them to be a seperate array\n",
    "train_container = ReviewContainer(train)\n",
    "train_container.evenly_distribute()\n",
    "\n",
    "train_text = train_container.get_sentence()   \n",
    "train_labels = train_container.get_propaganda() \n",
    "\n",
    "dev_text = [x.sentence for x in dev]\n",
    "dev_labels = [int(x.propaganda) for x in dev]\n",
    "\n",
    "test_text = [x.sentence for x in test]\n",
    "test_labels = [int(x.propaganda) for x in test]\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Save splited Data to seprate excel files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Excel file of train dataset\n",
    "train_df = pd.DataFrame(train_text, columns=[\"Sentence\"])\n",
    "train_df['SUBJprop'] = train_labels\n",
    "train_df.to_excel(\"./Data/trainDataset.xlsx\")\n",
    "# Create Excel file of dev dataset\n",
    "dev_df = pd.DataFrame(dev_text, columns=[\"Sentence\"])\n",
    "dev_df['SUBJprop'] = dev_labels\n",
    "dev_df['Tanbih'] = \"\"\n",
    "dev_df.to_excel(\"./Data/devDataset.xlsx\")\n",
    "# Create Excel file of test dataset\n",
    "test_df = pd.DataFrame(test_text, columns=[\"Sentence\"])\n",
    "test_df['SUBJprop'] = test_labels\n",
    "test_df.to_excel(\"./Data/testDataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    dev_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Integer Sequences to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 101, 2040, 2056,  ...,    0,    0,    0],\n        [ 101, 1999, 1996,  ...,  102,    0,    0],\n        [ 101, 6854, 1010,  ..., 1996, 6114,  102],\n        ...,\n        [ 101, 1999, 3218,  ..., 4491, 2000,  102],\n        [ 101, 1998, 2057,  ..., 2151, 4013,  102],\n        [ 101, 2130, 6715,  ..., 2031, 3622,  102]])\ntensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1]])\ntensor([0, 0, 0,  ..., 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels)\n",
    "\n",
    "\n",
    "\n",
    "# for validation set\n",
    "dev_seq = torch.tensor(tokens_val['input_ids'])\n",
    "dev_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "dev_y = torch.tensor(dev_labels)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels)\n",
    "\n",
    "print(test_seq)\n",
    "print(test_mask)\n",
    "print(test_y)"
   ]
  },
  {
   "source": [
    "# Classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='rbf',C=1, probability=True)\n",
    "clf_svm.fit(train_seq, train_y)\n",
    "\n",
    "clf_svm.predict(test_seq)"
   ]
  },
  {
   "source": [
    "## Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_seq , train_y)\n",
    "\n",
    "clf_dec.predict(test_seq)"
   ]
  },
  {
   "source": [
    "## Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(train_seq , train_y)\n",
    "\n",
    "clf_gnb.predict(test_seq)"
   ]
  },
  {
   "source": [
    "## Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_seq, train_y)\n",
    "\n",
    "clf_log.predict(test_seq)"
   ]
  },
  {
   "source": [
    "# Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# F1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.39683586 0.66148724]\n[0.40754717 0.6418251 ]\n[0.37074584 0.69390631]\n[0.43512748 0.59389002]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(f1_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Decision Tree\n",
    "print(f1_score(test_y,clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Naive Bayes\n",
    "print(f1_score(test_y,clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Logistic Regression\n",
    "print(f1_score(test_y,clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ]
  },
  {
   "source": [
    "## Mean Accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.566350710900474\n0.5535545023696683\n0.5881516587677725\n0.5274881516587677\n"
     ]
    }
   ],
   "source": [
    "# For Support Vector Machine\n",
    "print(clf_svm.score(test_seq,test_y))\n",
    "# For Decision Tree\n",
    "print(clf_dec.score(test_seq,test_y))\n",
    "# For Decision Naive Bayes\n",
    "print(clf_gnb.score(test_seq,test_y))\n",
    "# For Logistic Regression\n",
    "print(clf_log.score(test_seq,test_y))"
   ]
  },
  {
   "source": [
    "## Precision"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.32330827 0.75826972]\n[0.32270916 0.76311031]\n[0.32201258 0.74904943]\n[0.32569975 0.783029  ]\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(precision_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(precision_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(precision_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(precision_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "source": [
    "## Recall"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.51365188 0.58661417]\n[0.55290102 0.55380577]\n[0.43686007 0.64632546]\n[0.6552901  0.47834646]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(recall_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(recall_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(recall_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(recall_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "source": [
    "# VIP : Predict_Proba using Threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "AboveThresholdsvm = [] \n",
    "AboveThresholdsvmbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdsvm.append(test_text[i])\n",
    "    AboveThresholdsvmbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "print(len(AboveThresholdsvm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "AboveThresholdLR = [] \n",
    "AboveThresholdLRbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdLR.append(test_text[i])\n",
    "    AboveThresholdLRbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "print(len(AboveThresholdsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Bert\n",
    "tokens_testsvm = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdsvm,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqsvm = torch.tensor(tokens_testsvm['input_ids'])\n",
    "test_masksvm = torch.tensor(tokens_testsvm['attention_mask'])\n",
    "test_ysvm = torch.tensor(AboveThresholdsvmbrop)\n",
    "\n",
    "tokens_testLR = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdLR,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "\n",
    "test_seqLR = torch.tensor(tokens_testLR['input_ids'])\n",
    "test_maskLR = torch.tensor(tokens_testLR['attention_mask'])\n",
    "test_yLR = torch.tensor(AboveThresholdLRbrop)"
   ]
  },
  {
   "source": [
    "## Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 SVM more than 60% threshold\n[0.48275862 0.41340782]\nF1 LR more than 60% threshold\n[0.42827443 0.74607572]\n"
     ]
    }
   ],
   "source": [
    "#F1\n",
    "print(\"F1 SVM more than 60% threshold\")\n",
    "print(f1_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 LR more than 60% threshold\")\n",
    "print(f1_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy SVM more than 60% threshold\n0.450261780104712\nAccuracy LR more than 60% threshold\n0.6483375959079284\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "# For Support Vector Machine\n",
    "print(\"Accuracy SVM more than 60% threshold\")\n",
    "print(clf_svm.score(test_seqsvm,test_ysvm))\n",
    "print(\"Accuracy LR more than 60% threshold\")\n",
    "print(clf_log.score(test_seqLR,test_yLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision SVM more than 60% threshold\n[0.33333333 0.84090909]\nPrecision LR more than 60% threshold\n[0.3639576  0.80961924]\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "# For Support Vector Machine\n",
    "print(\"Precision SVM more than 60% threshold\")\n",
    "print(precision_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "print(\"Precision LR more than 60% threshold\")\n",
    "print(precision_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision SVM more than 60% threshold\n[0.875      0.27407407]\nPrecision LR more than 60% threshold\n[0.52020202 0.69178082]\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Precision SVM more than 60% threshold\")\n",
    "print(recall_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision LR more than 60% threshold\")\n",
    "print(recall_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}