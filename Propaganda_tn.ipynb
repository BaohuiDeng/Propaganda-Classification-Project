{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import All Packages\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "    import random\r\n",
    "    import pandas as pd\r\n",
    "    import numpy as np\r\n",
    "    import string\r\n",
    "    from string import digits\r\n",
    "    from sklearn.model_selection import train_test_split\r\n",
    "    import torch\r\n",
    "    import torch.nn as nn\r\n",
    "    from sklearn.metrics import classification_report\r\n",
    "    import transformers\r\n",
    "    from transformers import AutoModel, BertTokenizerFast\r\n",
    "    from ipywidgets import IntProgress\r\n",
    "    \r\n",
    "    from tqdm import tqdm\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import BERT Model, BERT Tokenizer and Torch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import BERT-base pretrained model\r\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "# Load the BERT tokenizer\r\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\r\n",
    "\r\n",
    "# import Torch\r\n",
    "device = torch.device(\"cpu\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predefined CLass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Propaganda:\r\n",
    "    NEGATIVE = 0\r\n",
    "    POSITIVE = 1\r\n",
    "class Review:\r\n",
    "    def __init__(self,sentence,SUBJprop):\r\n",
    "        self.sentence = sentence\r\n",
    "        self.SUBJprop = SUBJprop\r\n",
    "        self.propaganda = SUBJprop\r\n",
    "\r\n",
    "\r\n",
    "class ReviewContainer:\r\n",
    "    def __init__(self,reviews):\r\n",
    "        self.reviews = reviews\r\n",
    "\r\n",
    "    def get_sentence(self):\r\n",
    "        return [x.sentence for x  in self.reviews]\r\n",
    "\r\n",
    "    def get_propaganda(self):\r\n",
    "        return [int(x.propaganda) for x in self.reviews]\r\n",
    "\r\n",
    "    def evenly_distribute(self):\r\n",
    "        negative = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), self.reviews))\r\n",
    "        positive = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), self.reviews))\r\n",
    "        negative_shrunk = negative[:len(positive)]\r\n",
    "        self.reviews = positive + negative_shrunk\r\n",
    "        random.shuffle(self.reviews)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train dataset into train, validation and test sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# step 2.1: Load Data\r\n",
    "reviews = []\r\n",
    "data  = pd.read_excel('Data/finalDataset.xlsx', engine='openpyxl')\r\n",
    "df = pd.DataFrame(data.astype(str) , columns = ['Sentence','SUBJprop'])\r\n",
    "# iterate elements of attribute \"Sentence\" and \"SUBJprop\" and push to the array \"reviews\"\r\n",
    "\r\n",
    "for index, row in df.iterrows():\r\n",
    "    sentence = row['Sentence']\r\n",
    "    prop = row['SUBJprop']\r\n",
    "    reviews.append(Review(sentence,prop))\r\n",
    "\r\n",
    "\r\n",
    "print(\"Total Rows:\")\r\n",
    "print(len(reviews))\r\n",
    "print(\"Total Positive:\")\r\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))))\r\n",
    "print(\"Total Negative:\")\r\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Rows:\n",
      "14058\n",
      "Total Positive:\n",
      "3904\n",
      "Total Negative:\n",
      "10154\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# step 2.2: Load Sentimental Data\r\n",
    "SentimentWords= []\r\n",
    "SentimentValue= []\r\n",
    "\r\n",
    "\r\n",
    "Sentimentdata  = pd.read_excel('Data/SentimentalWords.xlsx', engine='openpyxl')\r\n",
    "df = pd.DataFrame(Sentimentdata.astype(str) , columns = ['word','value'])\r\n",
    "\r\n",
    "for index, row in df.iterrows():\r\n",
    "    word = row['word']\r\n",
    "    value = row['value']\r\n",
    "    SentimentWords.append(word)\r\n",
    "    SentimentValue.append(value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "neg_prop = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))\r\n",
    "pos_prop = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))\r\n",
    "########################################################################################\r\n",
    "#split trainig and DevTest dataset\r\n",
    "neg_train, neg_devtest  = train_test_split(neg_prop , train_size=0.7, shuffle= False )\r\n",
    "pos_train, pos_devtest = train_test_split(pos_prop , train_size=0.7, shuffle= False )\r\n",
    "########################################################################################\r\n",
    "#prepare training dataset\r\n",
    "train = neg_train + pos_train\r\n",
    "#random.shuffle(train)\r\n",
    "########################################################################################\r\n",
    "#prepare development and test dataset\r\n",
    "neg_dev, neg_test = train_test_split(neg_devtest , train_size=0.5, shuffle= False )\r\n",
    "pos_dev, pos_test = train_test_split(pos_devtest , train_size=0.5, shuffle= False )\r\n",
    "\r\n",
    "dev = neg_dev + pos_dev\r\n",
    "#random.shuffle(dev)\r\n",
    "\r\n",
    "test = neg_test + pos_test\r\n",
    "#random.shuffle(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# step 3: Seperate the attribute, originally our array has text and score. we want them to be a seperate array\r\n",
    "train_container = ReviewContainer(train)\r\n",
    "train_container.evenly_distribute()\r\n",
    "\r\n",
    "train_text = train_container.get_sentence()   \r\n",
    "train_labels = train_container.get_propaganda() \r\n",
    "\r\n",
    "dev_text = [x.sentence for x in dev]\r\n",
    "dev_labels = [int(x.propaganda) for x in dev]\r\n",
    "\r\n",
    "test_text = [x.sentence for x in test]\r\n",
    "test_labels = [int(x.propaganda) for x in test]\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save splited Data to seprate excel files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Create Excel file of train dataset\r\n",
    "train_df = pd.DataFrame(train_text, columns=[\"Sentence\"])\r\n",
    "train_df['SUBJprop'] = train_labels\r\n",
    "train_df.to_excel(\"./Data/trainDataset.xlsx\")\r\n",
    "# Create Excel file of dev dataset\r\n",
    "dev_df = pd.DataFrame(dev_text, columns=[\"Sentence\"])\r\n",
    "dev_df['SUBJprop'] = dev_labels\r\n",
    "dev_df['Tanbih'] = \"\"\r\n",
    "dev_df.to_excel(\"./Data/devDataset.xlsx\")\r\n",
    "# Create Excel file of test dataset\r\n",
    "test_df = pd.DataFrame(test_text, columns=[\"Sentence\"])\r\n",
    "test_df['SUBJprop'] = test_labels\r\n",
    "test_df.to_excel(\"./Data/testDataset.xlsx\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "max_seq_len = 20\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization and Filtering Punctuation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from nltk.tokenize import RegexpTokenizer\r\n",
    "tokenizerNLTK = RegexpTokenizer(r'\\w+')\r\n",
    "\r\n",
    "train_text_tokenized = []\r\n",
    "dev_text_tokenized = []\r\n",
    "test_text_tokenized = []\r\n",
    "\r\n",
    "\r\n",
    "for i in range(len(train_text)):\r\n",
    "    train_text_tokenized.append(tokenizerNLTK.tokenize(train_text[i])) \r\n",
    "\r\n",
    "for i in range(len(dev_text)):\r\n",
    "    dev_text_tokenized.append(tokenizerNLTK.tokenize(dev_text[i])) \r\n",
    "\r\n",
    "for i in range(len(test_text)):\r\n",
    "    test_text_tokenized.append(tokenizerNLTK.tokenize(test_text[i])) \r\n",
    "    \r\n",
    "\r\n",
    "print(test_text_tokenized[0:5])\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['WHO', 'said', 'there', 'should', 'not', 'be', 'restrictions', 'on', 'international', 'travel', 'or', 'trade'], ['In', 'the', 'meantime', 'it', 's', 'important', 'to', 'prevent', 'oneself', 'from', 'contracting', 'the', 'virus'], ['Unfortunately', 'health', 'care', 'workers', 'are', 'at', 'the', 'greatest', 'risk', 'as', 'they', 'try', 'to', 'cull', 'the', 'suffering', 'which', 'is', 'why', 'they', 'will', 'be', 'the', 'first', 'to', 'get', 'the', 'experimental', 'vaccine'], ['Pope', 'Francis', 'sexual', 'abuse', 'commission', 'hit', 'by', 'resignations', 'and', 'criticism', 'gets', 'a', 'reboot'], ['Last', 'week', 'Pope', 'Francis', 'announced', 'he', 'was', 'reviving', 'a', 'panel', 'he', 'created', 'to', 'advise', 'the', 'Vatican', 'on', 'how', 'to', 'handle', 'sexual', 'abuse', 'by', 'clergy']]\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Removing stop words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "set(stopwords.words('english'))\r\n",
    "from spacy.lang.en import English\r\n",
    "\r\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\r\n",
    "nlp = English()\r\n",
    "\r\n",
    "\r\n",
    "# Create list of word tokens after removing stopwords in train_text\r\n",
    "train_text1=[]\r\n",
    "for sentence in train_text_tokenized:\r\n",
    "    temp=[]\r\n",
    "    for word in sentence:\r\n",
    "        lexeme = nlp.vocab[word]\r\n",
    "        if lexeme.is_stop == False:\r\n",
    "            temp.append(word) \r\n",
    "    train_text1.append(temp)\r\n",
    "    \r\n",
    "# print(train_text1[:100])\r\n",
    "# print(\"*****************************************************\")\r\n",
    "\r\n",
    "# Create list of word tokens after removing stopwords in dev_text\r\n",
    "dev_text1 =[] \r\n",
    "\r\n",
    "for sentence in dev_text_tokenized:\r\n",
    "    temp=[]\r\n",
    "    for word in sentence:\r\n",
    "        lexeme = nlp.vocab[word]\r\n",
    "        if lexeme.is_stop == False:\r\n",
    "            temp.append(word) \r\n",
    "    dev_text1.append(temp)\r\n",
    "    \r\n",
    "# print(dev_text1[:100])\r\n",
    "# print(\"*****************************************************\")\r\n",
    "\r\n",
    "# # Create list of word tokens after removing stopwords in test_text\r\n",
    "test_text1 =[] \r\n",
    "\r\n",
    "for sentence in test_text_tokenized:\r\n",
    "    temp=[]\r\n",
    "    for word in sentence:\r\n",
    "        lexeme = nlp.vocab[word]\r\n",
    "        if lexeme.is_stop == False:\r\n",
    "            temp.append(word) \r\n",
    "    test_text1.append(temp)\r\n",
    "# print(test_text1[:100])  \r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Repetition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train_text2=[]\r\n",
    "# SentimentWords\r\n",
    "\r\n",
    "# def checkIfDuplicates(listOfElems):\r\n",
    "#     if len(listOfElems) == len(set(listOfElems)):\r\n",
    "#         return False\r\n",
    "#     else:\r\n",
    "#         return True\r\n",
    "\r\n",
    "# for sentence in train_text1:\r\n",
    "#     temp=[]\r\n",
    "#     if checkIfDuplicates(sentence):\r\n",
    "#         print(sentence)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    # for word in sentence:\r\n",
    "    #     temp.append(word)\r\n",
    "\r\n",
    "    # train_text2.append(temp)\r\n",
    "\r\n",
    "# print(train_text2[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Duplicate Sentimenal words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "train_text2=[]\r\n",
    "\r\n",
    "for sentence in train_text1:\r\n",
    "    temp=[]\r\n",
    "    for word in sentence:\r\n",
    "        temp.append(word)\r\n",
    "        for SentimentWord in SentimentWords:\r\n",
    "            if word.lower()==SentimentWord:\r\n",
    "                temp.append(word)\r\n",
    "                temp.append(word)\r\n",
    "\r\n",
    "    train_text2.append(temp)\r\n",
    "\r\n",
    "print(train_text2[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['important', 'theology', 'sin'], ['Editor', 's', 'Note', 'Remnant', 'lunched', 'StopTheSynod', 'petition', 'Change', 'org'], ['seriously', 'sanctity', 'institution', 'individuals', 'continue', 'suffer', 'wages', 'sin'], ['terrible', 'insult', 'spark', 'decisions', 'Japan', 'proceed', 'ahead', 'covert', 'nuclear', 'program'], ['art', 'deal'], ['depends'], ['Rush', 'Limbaugh', 'points', 'Senate', 'bent', 'backwards', 'accommodate', 'accuser'], ['Khecharem', 'predicted', 'happen', 'course', 'elimination', 'filthy', 'Jewish', 'entity', 'liberation', 'lands', 'direct', 'colonization', 'like', 'Kashmir'], ['Concerned', 'clergy', 'laity', 'Catholic', 'world', 'according', 'stations'], ['Ms', 'Ford', 's', 'unsubstantiated', 'accusation', 'completely', 'destroy', 'Judge', 'Kavanaugh', 's', 'life', 'causing', 'irreparable', 'damage', 'reputation', 'integrity', 'good', 'character', 'career', 'built', 'decades', 'public', 'service']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Detokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_text_detokenized=[]\r\n",
    "for i in range(len(train_text2)):\r\n",
    "    temp=\"\"\r\n",
    "    for j in range(len(train_text2[i])):\r\n",
    "        temp=temp + \" \" + train_text2[i][j]        \r\n",
    "    train_text_detokenized.append(temp)\r\n",
    "# print(train_text_detokenized[:100])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "dev_text_detokenized=[]\r\n",
    "for i in range(len(dev_text1)):\r\n",
    "    temp=\"\"\r\n",
    "    for j in range(len(dev_text1[i])):\r\n",
    "        temp=temp + \" \" + dev_text1[i][j]        \r\n",
    "    dev_text_detokenized.append(temp)\r\n",
    "# print(dev_text_detokenized[:100])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "test_text_detokenized=[]\r\n",
    "for i in range(len(test_text1)):\r\n",
    "    temp=\"\"\r\n",
    "    for j in range(len(test_text1[i])):\r\n",
    "        temp=temp + \" \" + test_text1[i][j]        \r\n",
    "    test_text_detokenized.append(temp)\r\n",
    "print(test_text_detokenized[:100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[' said restrictions international travel trade', ' meantime s important prevent oneself contracting virus', ' Unfortunately health care workers greatest risk try cull suffering experimental vaccine', ' Pope Francis sexual abuse commission hit resignations criticism gets reboot', ' week Pope Francis announced reviving panel created advise Vatican handle sexual abuse clergy', ' issue dogged Roman Catholic Church recent years critics accused Francis predecessors John Paul II Benedict XVI failing aggressively weed punish predator priests', ' s look drove panel s creation expect future', ' Pope Francis commission sexual abuse', ' Following criticism focusing halting child abuse church Francis created Pontifical Commission Protection Minors March 2014 naming Cardinal Sean O Malley archbishop Boston run', ' O Malley member pope s G9 group close advisors man sought clean abuse scandal Boston departure predecessor Cardinal Bernard Law shifting predator priests new posts exposed Boston Globe chronicled movie Spotlight', ' revelations Law died December emboldened abuse victims country world began speaking priests molested', ' created commission Francis said commission s specific task propose opportune initiatives protecting minors vulnerable adults order possible ensure crimes occurred longer repeated Church', ' renewed', ' commission s year mandate expired December Francis relaunched Feb 17 members reappointed new members added victims abuse Vatican declined opting respect privacy', ' O Malley returns chief', ' New members come Australia Brazil Ethiopia India Netherlands Tonga reflecting global reach church challenge creating safeguarding structures diverse cultural context Vatican said', ' commission achieve', ' Vatican said worked 200 dioceses religious communities worldwide raise awareness educate people need safeguarding homes parishes schools hospitals institutions', ' didn t', ' Commission members quickly realized needed raising awareness deal bishops quietly shift offenders new dioceses exposed molesters rapists', ' church laws existed hold bishops accountable priests behavior strictures O Malley said 2014', ' theoretically guess canons apply obviously sufficient said', ' amid myriad legal questions exact powers court got ground', ' 2016 Francis officially killed plan create issuing document essentially called existing procedures tackle problem', ' Francis criticized Australian Cardinal George Pell brought Rome clean Vatican finances forced return Australia year face charges sexual assault', ' problems', ' simple message hard', ' emerged Vatican training course new bishops September 2015 French priest informed bishops obligation report abuse police forcing commission issue strong statement saying bishops obligation', ' commission members persevere', '', ' British member Peter Saunders victim priestly abuse ousted 2016', ' Irish member Marie Collins victim resigned March year said Vatican bureaucrats refused comply commission s request approved pope respond letters sent Vatican abuse victims', ' Marie Collins member pope s sex abuse commission 2015 hands letter Cardinal Sean O Malley detailing abuse suffered Juan Carlos Cruz Chile', ' Catherine Bonnet Associated Press', ' pope listening commission', ' Possibly', ' 2015 Collins handed O Malley letter addressed pope Juan Carlos Cruz Chilean abused boy Chilean prelate Fernando Karadima sentenced Vatican lifetime penance 2010', ' Cruz described abuse witnessed covered Juan Barros priest appointment bishop 2015 prompted huge protests Chile', ' t condemn Barros don t evidence said plane Rome suggesting ignored read letter Cruz', ' Vatican watchers believe pope influenced Chilean Cardinal Francisco Javier Errazuriz member G9 group backed Barros reportedly helped block moves Cruz member abuse commission', ' pope intransigent Barros', '', ' changed mind Chilean trip possibly scale protest Barros country', ' Feb 17 sent senior Vatican sex abuse investigator Archbishop Charles Scicluna New York interview Cruz lives Philadelphia', ' time felt listening Cruz said meeting', ' happens', ' lead Barros stepping said Saunders forming campaign group Cruz fight clerical abuse', ' new papal commission hold meeting April', ' Support journalism consider subscribing today support stories like', ' access signature journalism 99 cents weeks', ' subscriber', ' support makes work possible', ' Thank', ' Kington special correspondent', ' rumor Dina Habib Powell candidate replace outgoing U S', ' hopes rumor mill wrong', ' Dina Habib Powell deep Republican establishment', ' 2017 resigned post Deputy National Security Adviser NSC Islamic apologist H R McMaster failed views shared', ' McMaster subscribed Obama view Islamic terrorism Islam despite numerous Islamic texts teachings incite Muslims wage war unbelievers', ' McMaster went far claim devout jihadis irreligious', ' poll story continues', ' replace Nikki Haley ambassador U N', ' replace Nikki Haley ambassador U N', ' replace Nikki Haley ambassador U N', ' John Bolton Richard Grenell Dina Powell Heather Nauert Ivanka Trump', ' Email', ' Phone field validation purposes left unchanged', ' Completing poll grants access Freedom Outpost updates free charge', ' opt anytime', ' agree site s Privacy Policy Terms Use', ' McMaster forced superb K T', ' President Trump finally replaced national security adviser H R', ' President Trump', ' visiting Egypt Habib Powell assured locals Bush September 11 visited mosque took shoes paid respects president talk Islam religion peace host iftar year gushed', ' K T', ' McFarland written Global Islamist jihad war Western civilization s hard McMaster pushed McFarland elevated Habib Powell', ' Habib Powell attended Iftar dinner members Muslim Brotherhood groups', ' photographed American Task Force Palestine gala', ' ATFP originally Rashid Khalidi s American Committee Jerusalem', ' presenter Middle East Institute speech Hanan Ashrawi', ' achievements Bush included cultural exchanges Iran cash Palestinian Authority Lebanon Hezbollah war Israel', ' President Trump fights restrict Muslim immigration woman bragged onCNN 90 student visas issued week Middle East CFP', ' separate National Security Council sources confirmed National Security Advisor H R McMaster Deputy National Security Advisor Dina Powell leaking negative material President Donald J Trump allies anti Trump media', ' Daniel Greenfield said Dina Habib Powell media dubbed Republican Huma Abedin', ' s powerful women Republican administrations', ' s friends Valerie Jarrett Habib Powell right friends', ' Like Valerie Jarrett', ' Arianna Huffington praised White House bringing', ' ex husband heads Teneo Strategy organization created man Clinton Foundation happen employed Huma Abedin', ' posing Huma Arianna Saudi princess', ' photographed American Task Force Palestine gala', ' ATFP originally Rashid Khalidi s American Committee Jerusalem', ' Khalidi PLO spokesman center Obama tape scandal', ' Habib Powell presenter Middle East Institute speech PLO s Hanan Ashrawi Dina Habib Powell deep Republican establishment', ' role NSC represents McMaster s vision approach Islam', ' s echo failed approach Bush years', ' Flynn NSC tool matched Trump s vision', ' McMaster remaking match Jeb Bush s vision', ' Powell close ties Clintons', ' Breitbart']\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\r\n",
    "# tokenize and encode sequences in the training set\r\n",
    "tokens_train = tokenizer.batch_encode_plus(\r\n",
    "    train_text_detokenized,\r\n",
    "    max_length = max_seq_len,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    truncation=True,\r\n",
    "    return_token_type_ids=False\r\n",
    ")\r\n",
    "\r\n",
    "# tokenize and encode sequences in the validation set\r\n",
    "tokens_val = tokenizer.batch_encode_plus(\r\n",
    "    dev_text_detokenized,\r\n",
    "    max_length = max_seq_len,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    truncation=True,\r\n",
    "    return_token_type_ids=False\r\n",
    ")\r\n",
    "\r\n",
    "# tokenize and encode sequences in the test set\r\n",
    "tokens_test = tokenizer.batch_encode_plus(\r\n",
    "    test_text_detokenized,\r\n",
    "    max_length = max_seq_len,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    truncation=True,\r\n",
    "    return_token_type_ids=False\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tanna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert Integer Sequences to Tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# for train set\r\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\r\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\r\n",
    "train_y = torch.tensor(train_labels)\r\n",
    "\r\n",
    "print(train_seq[0])\r\n",
    "\r\n",
    "# for validation set\r\n",
    "dev_seq = torch.tensor(tokens_val['input_ids'])\r\n",
    "dev_mask = torch.tensor(tokens_val['attention_mask'])\r\n",
    "dev_y = torch.tensor(dev_labels)\r\n",
    "\r\n",
    "# for test set\r\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\r\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\r\n",
    "test_y = torch.tensor(test_labels)\r\n",
    "\r\n",
    "\r\n",
    "print(test_seq)\r\n",
    "print(test_mask)\r\n",
    "print(test_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 101, 2590, 8006, 8254,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "tensor([[  101,  2056,  9259,  ...,     0,     0,     0],\n",
      "        [  101, 12507,  1055,  ...,     0,     0,     0],\n",
      "        [  101,  6854,  2740,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  3218, 16263,  ...,     0,     0,     0],\n",
      "        [  101,  2113,  3599,  ...,     0,     0,     0],\n",
      "        [  101,  6715,  2015,  ...,     0,     0,     0]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "tensor([0, 0, 0,  ..., 1, 1, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\r\n",
    "from sklearn import svm\r\n",
    "\r\n",
    "clf_svm = svm.SVC(kernel='rbf',C=1, probability=True)\r\n",
    "clf_svm.fit(train_seq, train_y)\r\n",
    "\r\n",
    "# clf_svm.predict(test_seq)\r\n",
    "# print(len(test_seq))\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(C=1, probability=True)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "\r\n",
    "clf_dec = DecisionTreeClassifier()\r\n",
    "clf_dec.fit(train_seq , train_y)\r\n",
    "\r\n",
    "clf_dec.predict(test_seq)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\r\n",
    "\r\n",
    "clf_gnb = GaussianNB()\r\n",
    "clf_gnb.fit(train_seq , train_y)\r\n",
    "\r\n",
    "clf_gnb.predict(test_seq)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "\r\n",
    "clf_log = LogisticRegression()\r\n",
    "clf_log.fit(train_seq, train_y)\r\n",
    "\r\n",
    "clf_log.predict(test_seq)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# F1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "# For Support Vector Machine\r\n",
    "print(f1_score(test_y, clf_svm.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "\r\n",
    "# For Support Decision Tree\r\n",
    "print(f1_score(test_y,clf_dec.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "\r\n",
    "# For Support Naive Bayes\r\n",
    "print(f1_score(test_y,clf_gnb.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "\r\n",
    "# For Logistic Regression\r\n",
    "print(f1_score(test_y,clf_log.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.41007698 0.69795772]\n",
      "[0.40204212 0.64681493]\n",
      "[0.37735849 0.73134328]\n",
      "[0.39106145 0.68723099]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# For Support Vector Machine\r\n",
    "print(clf_svm.score(test_seq,test_y))\r\n",
    "# For Decision Tree\r\n",
    "print(clf_dec.score(test_seq,test_y))\r\n",
    "# For Decision Naive Bayes\r\n",
    "print(clf_gnb.score(test_seq,test_y))\r\n",
    "# For Logistic Regression\r\n",
    "print(clf_log.score(test_seq,test_y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6004739336492891\n",
      "0.5559241706161138\n",
      "0.6246445497630332\n",
      "0.5867298578199052\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Precision"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Precision\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "\r\n",
    "# For Support Vector Machine\r\n",
    "print(precision_score(test_y, clf_svm.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "# For Decision Tree\r\n",
    "print(precision_score(test_y, clf_dec.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "# For Decision Naive Bayes\r\n",
    "print(precision_score(test_y, clf_gnb.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "# For Logistic Regression\r\n",
    "print(precision_score(test_y, clf_log.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.34756821 0.76874507]\n",
      "[0.32110092 0.75996457]\n",
      "[0.34985423 0.75702247]\n",
      "[0.33096927 0.75791139]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recall"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from sklearn.metrics import recall_score\r\n",
    "\r\n",
    "# For Support Vector Machine\r\n",
    "print(recall_score(test_y, clf_svm.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "# For Decision Tree\r\n",
    "print(recall_score(test_y, clf_dec.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "# For Decision Naive Bayes\r\n",
    "print(recall_score(test_y, clf_gnb.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "# For Logistic Regression\r\n",
    "print(recall_score(test_y, clf_log.predict(test_seq),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.5        0.63910761]\n",
      "[0.53754266 0.56299213]\n",
      "[0.40955631 0.70734908]\n",
      "[0.4778157  0.62860892]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VIP : Predict_Proba using Threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "AboveThresholdsvm = [] \r\n",
    "AboveThresholdsvmbrop = []\r\n",
    "# [0][1] = Postivie\r\n",
    "# [0][0] = Negative\r\n",
    "\r\n",
    "\r\n",
    "for i in range(len(test_seq)):\r\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\r\n",
    "    AboveThresholdsvm.append(test_text[i])\r\n",
    "    AboveThresholdsvmbrop.append(test_labels[i])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(len(AboveThresholdsvm))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "466\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "AboveThresholdLR = [] \r\n",
    "AboveThresholdLRbrop = []\r\n",
    "# [0][1] = Postivie\r\n",
    "# [0][0] = Negative\r\n",
    "\r\n",
    "\r\n",
    "for i in range(len(test_seq)):\r\n",
    "  if clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\r\n",
    "    AboveThresholdLR.append(test_text[i])\r\n",
    "    AboveThresholdLRbrop.append(test_labels[i])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "print(len(AboveThresholdLR))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "75\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Bert\r\n",
    "tokens_testsvm = tokenizer.batch_encode_plus(\r\n",
    "    AboveThresholdsvm,\r\n",
    "    max_length = max_seq_len,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    truncation=True,\r\n",
    "    return_token_type_ids=False\r\n",
    ")\r\n",
    "test_seqsvm = torch.tensor(tokens_testsvm['input_ids'])\r\n",
    "test_masksvm = torch.tensor(tokens_testsvm['attention_mask'])\r\n",
    "test_ysvm = torch.tensor(AboveThresholdsvmbrop)\r\n",
    "\r\n",
    "tokens_testLR = tokenizer.batch_encode_plus(\r\n",
    "    AboveThresholdLR,\r\n",
    "    max_length = max_seq_len,\r\n",
    "    pad_to_max_length=True,\r\n",
    "    truncation=True,\r\n",
    "    return_token_type_ids=False\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "test_seqLR = torch.tensor(tokens_testLR['input_ids'])\r\n",
    "test_maskLR = torch.tensor(tokens_testLR['attention_mask'])\r\n",
    "test_yLR = torch.tensor(AboveThresholdLRbrop)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\tanna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#F1\r\n",
    "print(\"F1 SVM more than 60% threshold\")\r\n",
    "print(f1_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "\r\n",
    "print(\"F1 LR more than 60% threshold\")\r\n",
    "print(f1_score(test_yLR, clf_log.predict(test_seqLR),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 SVM more than 60% threshold\n",
      "[0.43577982 0.50403226]\n",
      "F1 LR more than 60% threshold\n",
      "[0.388      0.71823204]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Accuracy\r\n",
    "# For Support Vector Machine\r\n",
    "print(\"Accuracy SVM more than 60% threshold\")\r\n",
    "print(clf_svm.score(test_seqsvm,test_ysvm))\r\n",
    "print(\"Accuracy LR more than 60% threshold\")\r\n",
    "print(clf_log.score(test_seqLR,test_yLR))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy SVM more than 60% threshold\n",
      "0.4721030042918455\n",
      "Accuracy LR more than 60% threshold\n",
      "0.6141235813366961\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Precision\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "# For Support Vector Machine\r\n",
    "print(\"Precision SVM more than 60% threshold\")\r\n",
    "print(precision_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "print(\"Precision LR more than 60% threshold\")\r\n",
    "print(precision_score(test_yLR, clf_log.predict(test_seqLR),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision SVM more than 60% threshold\n",
      "[0.33807829 0.67567568]\n",
      "Precision LR more than 60% threshold\n",
      "[0.34892086 0.75728155]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Recall\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "print(\"Precision SVM more than 60% threshold\")\r\n",
    "print(recall_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\r\n",
    "\r\n",
    "print(\"Precision LR more than 60% threshold\")\r\n",
    "print(recall_score(test_yLR, clf_log.predict(test_seqLR),average = None, \r\n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision SVM more than 60% threshold\n",
      "[0.61290323 0.40192926]\n",
      "Precision LR more than 60% threshold\n",
      "[0.43693694 0.68301226]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "Output=[]\r\n",
    "for i in range(len(test_seq)):\r\n",
    "  Vote=0\r\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\r\n",
    "    Vote=Vote+1\r\n",
    "  elif clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\r\n",
    "    Vote=Vote+1\r\n",
    "  elif clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\r\n",
    "    Vote=Vote+1\r\n",
    "  elif clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\r\n",
    "    Vote=Vote+1\r\n",
    "\r\n",
    "\r\n",
    "  if (Vote>1):\r\n",
    "    Output.append(\"Propaganda\")\r\n",
    "  else:\r\n",
    "    Output.append(\"nonPropaganda\")\r\n",
    "\r\n",
    "\r\n",
    "print(Output[:200])\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_seq' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aacdae6606e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mOutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[0mVote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mclf_svm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0.60\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mVote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVote\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_seq' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "CountPropaganda=0\r\n",
    "for i in range(len(Output)):\r\n",
    "    if (Output[i]==\"Propaganda\"):\r\n",
    "        CountPropaganda=CountPropaganda+1\r\n",
    "\r\n",
    "print(\"number of propandas in output:\")\r\n",
    "print(CountPropaganda)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of propandas in output:\n",
      "0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18c678232501d9e663cd871fe97be1101dcc5092fd6c303aa08f6d7048afc0a5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}