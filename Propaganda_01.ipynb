{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Packages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import random\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import string\n",
    "    from string import digits\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import classification_report\n",
    "    import transformers\n",
    "    from transformers import AutoModel, BertTokenizerFast\n",
    "    from ipywidgets import IntProgress\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT Model, BERT Tokenizer and Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# import Torch\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predefined CLass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Propaganda:\r\n",
    "    NEGATIVE = 0\r\n",
    "    POSITIVE = 1\r\n",
    "class Review:\r\n",
    "    def __init__(self,sentence,SUBJprop):\r\n",
    "        self.sentence = sentence\r\n",
    "        self.SUBJprop = SUBJprop\r\n",
    "        self.propaganda = SUBJprop\r\n",
    "\r\n",
    "\r\n",
    "class ReviewContainer:\r\n",
    "    def __init__(self,reviews):\r\n",
    "        self.reviews = reviews\r\n",
    "\r\n",
    "    def get_sentence(self):\r\n",
    "        return [x.sentence for x  in self.reviews]\r\n",
    "\r\n",
    "    def get_propaganda(self):\r\n",
    "        return [int(x.propaganda) for x in self.reviews]\r\n",
    "\r\n",
    "    def evenly_distribute(self):\r\n",
    "        negative = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), self.reviews))\r\n",
    "        positive = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), self.reviews))\r\n",
    "        negative_shrunk = negative[:len(positive)]\r\n",
    "        self.reviews = positive + negative_shrunk\r\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2.1: Load Data\r\n",
    "reviews = []\r\n",
    "data  = pd.read_excel('Data/finalDataset.xlsx', engine='openpyxl')\r\n",
    "df = pd.DataFrame(data.astype(str) , columns = ['Sentence','SUBJprop'])\r\n",
    "# iterate elements of attribute \"Sentence\" and \"SUBJprop\" and push to the array \"reviews\"\r\n",
    "\r\n",
    "for index, row in df.iterrows():\r\n",
    "    sentence = row['Sentence']\r\n",
    "    prop = row['SUBJprop']\r\n",
    "    reviews.append(Review(sentence,prop))\r\n",
    "\r\n",
    "\r\n",
    "print(\"Total Rows:\")\r\n",
    "print(len(reviews))\r\n",
    "print(\"Total Positive:\")\r\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))))\r\n",
    "print(\"Total Negative:\")\r\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2.2: Load Sentimental Data\r\n",
    "SentimentWords= []\r\n",
    "SentimentValue= []\r\n",
    "\r\n",
    "\r\n",
    "Sentimentdata  = pd.read_excel('Data/SentimentalWords.xlsx', engine='openpyxl')\r\n",
    "df = pd.DataFrame(Sentimentdata.astype(str) , columns = ['word','value'])\r\n",
    "\r\n",
    "for index, row in df.iterrows():\r\n",
    "    word = row['word']\r\n",
    "    value = row['value']\r\n",
    "    SentimentWords.append(word)\r\n",
    "    SentimentValue.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_prop = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))\n",
    "pos_prop = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))\n",
    "########################################################################################\n",
    "#split trainig and DevTest dataset\n",
    "neg_train, neg_devtest  = train_test_split(neg_prop , train_size=0.7, shuffle= False )\n",
    "pos_train, pos_devtest = train_test_split(pos_prop , train_size=0.7, shuffle= False )\n",
    "########################################################################################\n",
    "#prepare training dataset\n",
    "train = neg_train + pos_train\n",
    "#random.shuffle(train)\n",
    "########################################################################################\n",
    "#prepare development and test dataset\n",
    "neg_dev, neg_test = train_test_split(neg_devtest , train_size=0.5, shuffle= False )\n",
    "pos_dev, pos_test = train_test_split(pos_devtest , train_size=0.5, shuffle= False )\n",
    "\n",
    "dev = neg_dev + pos_dev\n",
    "#random.shuffle(dev)\n",
    "\n",
    "test = neg_test + pos_test\n",
    "#random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: Seperate the attribute, originally our array has text and score. we want them to be a seperate array\n",
    "train_container = ReviewContainer(train)\n",
    "train_container.evenly_distribute()\n",
    "\n",
    "train_text = train_container.get_sentence()   \n",
    "train_labels = train_container.get_propaganda() \n",
    "\n",
    "dev_text = [x.sentence for x in dev]\n",
    "dev_labels = [int(x.propaganda) for x in dev]\n",
    "\n",
    "test_text = [x.sentence for x in test]\n",
    "test_labels = [int(x.propaganda) for x in test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save splited Data to seprate excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Excel file of train dataset\n",
    "train_df = pd.DataFrame(train_text, columns=[\"Sentence\"])\n",
    "train_df['SUBJprop'] = train_labels\n",
    "train_df.to_excel(\"./Data/trainDataset.xlsx\")\n",
    "# Create Excel file of dev dataset\n",
    "dev_df = pd.DataFrame(dev_text, columns=[\"Sentence\"])\n",
    "dev_df['SUBJprop'] = dev_labels\n",
    "dev_df['Tanbih'] = \"\"\n",
    "dev_df.to_excel(\"./Data/devDataset.xlsx\")\n",
    "# Create Excel file of test dataset\n",
    "test_df = pd.DataFrame(test_text, columns=[\"Sentence\"])\n",
    "test_df['SUBJprop'] = test_labels\n",
    "test_df.to_excel(\"./Data/testDataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Filtering Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizerNLTK = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "train_text_tokenized = []\n",
    "dev_text_tokenized = []\n",
    "test_text_tokenized = []\n",
    "\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "    train_text_tokenized.append(tokenizerNLTK.tokenize(train_text[i])) \n",
    "\n",
    "for i in range(len(dev_text)):\n",
    "    dev_text_tokenized.append(tokenizerNLTK.tokenize(dev_text[i])) \n",
    "\n",
    "for i in range(len(test_text)):\n",
    "    test_text_tokenized.append(tokenizerNLTK.tokenize(test_text[i])) \n",
    "    \n",
    "\n",
    "print(test_text_tokenized[0:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "set(stopwords.words('english'))\r\n",
    "from spacy.lang.en import English\r\n",
    "\r\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\r\n",
    "nlp = English()\r\n",
    "\r\n",
    "\r\n",
    "# Create list of word tokens after removing stopwords in train_text\r\n",
    "train_text1=[]\r\n",
    "for sentence in train_text_tokenized:\r\n",
    "    temp=[]\r\n",
    "    for word in sentence:\r\n",
    "        lexeme = nlp.vocab[word]\r\n",
    "        if lexeme.is_stop == False:\r\n",
    "            temp.append(word) \r\n",
    "    train_text1.append(temp)\r\n",
    "    \r\n",
    "# print(train_text1[:100])\r\n",
    "# print(\"*****************************************************\")\r\n",
    "\r\n",
    "# Create list of word tokens after removing stopwords in dev_text\r\n",
    "dev_text1 =[] \r\n",
    "\r\n",
    "for sentence in dev_text_tokenized:\r\n",
    "    temp=[]\r\n",
    "    for word in sentence:\r\n",
    "        lexeme = nlp.vocab[word]\r\n",
    "        if lexeme.is_stop == False:\r\n",
    "            temp.append(word) \r\n",
    "    dev_text1.append(temp)\r\n",
    "    \r\n",
    "# print(dev_text1[:100])\r\n",
    "# print(\"*****************************************************\")\r\n",
    "\r\n",
    "# # Create list of word tokens after removing stopwords in test_text\r\n",
    "test_text1 =[] \r\n",
    "\r\n",
    "for sentence in test_text_tokenized:\r\n",
    "    temp=[]\r\n",
    "    for word in sentence:\r\n",
    "        lexeme = nlp.vocab[word]\r\n",
    "        if lexeme.is_stop == False:\r\n",
    "            temp.append(word) \r\n",
    "    test_text1.append(temp)\r\n",
    "# print(test_text1[:100])  \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_text2=[]\r\n",
    "# SentimentWords\r\n",
    "\r\n",
    "# def checkIfDuplicates(listOfElems):\r\n",
    "#     if len(listOfElems) == len(set(listOfElems)):\r\n",
    "#         return False\r\n",
    "#     else:\r\n",
    "#         return True\r\n",
    "\r\n",
    "# for sentence in train_text1:\r\n",
    "#     temp=[]\r\n",
    "#     if checkIfDuplicates(sentence):\r\n",
    "#         print(sentence)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    # for word in sentence:\r\n",
    "    #     temp.append(word)\r\n",
    "\r\n",
    "    # train_text2.append(temp)\r\n",
    "\r\n",
    "# print(train_text2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Sentimenal words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text2=[]\r\n",
    "\r\n",
    "for sentence in train_text1:\r\n",
    "    temp=[]\r\n",
    "    for word in sentence:\r\n",
    "        temp.append(word)\r\n",
    "        for SentimentWord in SentimentWords:\r\n",
    "            if word.lower()==SentimentWord:\r\n",
    "                temp.append(word)\r\n",
    "\r\n",
    "    train_text2.append(temp)\r\n",
    "\r\n",
    "print(train_text2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_text_detokenized=[]\n",
    "for i in range(len(train_text1)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(train_text1[i])):\n",
    "        temp=temp + \" \" + train_text1[i][j]        \n",
    "    train_text_detokenized.append(temp)\n",
    "# print(train_text_detokenized[:100])\n",
    "\n",
    "\n",
    "\n",
    "dev_text_detokenized=[]\n",
    "for i in range(len(dev_text1)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(dev_text1[i])):\n",
    "        temp=temp + \" \" + dev_text1[i][j]        \n",
    "    dev_text_detokenized.append(temp)\n",
    "# print(dev_text_detokenized[:100])\n",
    "\n",
    "\n",
    "\n",
    "test_text_detokenized=[]\n",
    "for i in range(len(test_text1)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(test_text1[i])):\n",
    "        temp=temp + \" \" + test_text1[i][j]        \n",
    "    test_text_detokenized.append(temp)\n",
    "print(test_text_detokenized[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text_detokenized,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    dev_text_detokenized,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text_detokenized,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Integer Sequences to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels)\n",
    "\n",
    "print(train_seq[0])\n",
    "\n",
    "# for validation set\n",
    "dev_seq = torch.tensor(tokens_val['input_ids'])\n",
    "dev_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "dev_y = torch.tensor(dev_labels)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels)\n",
    "\n",
    "\n",
    "print(test_seq)\n",
    "print(test_mask)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='rbf',C=1, probability=True)\n",
    "clf_svm.fit(train_seq, train_y)\n",
    "\n",
    "# clf_svm.predict(test_seq)\n",
    "# print(len(test_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier()\n",
    "clf_dec.fit(train_seq , train_y)\n",
    "\n",
    "clf_dec.predict(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB()\n",
    "clf_gnb.fit(train_seq , train_y)\n",
    "\n",
    "clf_gnb.predict(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_seq, train_y)\n",
    "\n",
    "clf_log.predict(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(f1_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Decision Tree\n",
    "print(f1_score(test_y,clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Naive Bayes\n",
    "print(f1_score(test_y,clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Logistic Regression\n",
    "print(f1_score(test_y,clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Support Vector Machine\n",
    "print(clf_svm.score(test_seq,test_y))\n",
    "# For Decision Tree\n",
    "print(clf_dec.score(test_seq,test_y))\n",
    "# For Decision Naive Bayes\n",
    "print(clf_gnb.score(test_seq,test_y))\n",
    "# For Logistic Regression\n",
    "print(clf_log.score(test_seq,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(precision_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(precision_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(precision_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(precision_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(recall_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(recall_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(recall_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(recall_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIP : Predict_Proba using Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AboveThresholdsvm = [] \n",
    "AboveThresholdsvmbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdsvm.append(test_text[i])\n",
    "    AboveThresholdsvmbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "print(len(AboveThresholdsvm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AboveThresholdLR = [] \n",
    "AboveThresholdLRbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdLR.append(test_text[i])\n",
    "    AboveThresholdLRbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "\n",
    "print(len(AboveThresholdsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert\n",
    "tokens_testsvm = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdsvm,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqsvm = torch.tensor(tokens_testsvm['input_ids'])\n",
    "test_masksvm = torch.tensor(tokens_testsvm['attention_mask'])\n",
    "test_ysvm = torch.tensor(AboveThresholdsvmbrop)\n",
    "\n",
    "tokens_testLR = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdLR,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "\n",
    "test_seqLR = torch.tensor(tokens_testLR['input_ids'])\n",
    "test_maskLR = torch.tensor(tokens_testLR['attention_mask'])\n",
    "test_yLR = torch.tensor(AboveThresholdLRbrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1\n",
    "print(\"F1 SVM more than 60% threshold\")\n",
    "print(f1_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 LR more than 60% threshold\")\n",
    "print(f1_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "# For Support Vector Machine\n",
    "print(\"Accuracy SVM more than 60% threshold\")\n",
    "print(clf_svm.score(test_seqsvm,test_ysvm))\n",
    "print(\"Accuracy LR more than 60% threshold\")\n",
    "print(clf_log.score(test_seqLR,test_yLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "# For Support Vector Machine\n",
    "print(\"Precision SVM more than 60% threshold\")\n",
    "print(precision_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "print(\"Precision LR more than 60% threshold\")\n",
    "print(precision_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Precision SVM more than 60% threshold\")\n",
    "print(recall_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision LR more than 60% threshold\")\n",
    "print(recall_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18c678232501d9e663cd871fe97be1101dcc5092fd6c303aa08f6d7048afc0a5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}